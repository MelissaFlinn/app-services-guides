////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-connectors-rhoas-cli"]
= Getting started with the rhoas CLI for {product-long-connectors}
ifdef::context[:parent-context: {context}]
:context: connectors-rhoas-cli

// Purpose statement for the assembly
[role="_abstract"]
As a developer of {product-connectors}, you can use the `rhoas` command-line interface (CLI) to create and configure connections between {product-long-kafka} and third-party systems.

You can use a *source* connector to send data from an external system to {product-kafka}. You can use a *sink* connector to send data from {product-kafka} to an external system.

For the example in this guide, you create a source connector that sends data from a simple data generator to a Kafka topic. You also create a sink connector that sends data from the Kafka topic to an HTTP site.

Use this guide to complete the following tasks:

* {base-url}{getting-started-rhoas-cli-url-connectors}#proc-create-connector-namespace_connectors-rhoas-cli[Create a namespace to host your {product-connectors} instances.]
* {base-url}{getting-started-rhoas-cli-url-connectors}#proc-building-connector-configuration-cli_connectors-rhoas-cli[Build a configuration file for each type of connector that you want to create]
* {base-url}{getting-started-rhoas-cli-url-connectors}#proc-create-connector-instances_connectors-rhoas-cli[Create a {product-connectors} instance by specifying a configuration file]

.Prerequisites

* You have a Red Hat account.
* You've installed the latest version of the `rhoas` CLI. See {base-url}{installation-guide-url-cli}[Installing and configuring the rhoas CLI^].
* You've completed the following tasks:
+
*Note:* You can find detailed instructions for these tasks in {base-url}{getting-started-rhoas-cli-url-kafka}[Getting started with the rhoas CLI for {product-long-kafka}^].

** Create a Kafka instance
[source,subs="+quotes"]
+
----
$ rhoas kafka create --name my-kafka-instance 
----

** Verify that the Kafka instance is in the *Ready* state.
+
[source,subs="+quotes"]
----
$ rhoas context status kafka 
----

** Create a Kafka topic named `test-topic`. The Kafka topic stores messages sent by producers (data sources) and makes them available to consumers (data sinks).
+
[source,subs="+quotes"]
----
$ rhoas kafka topic create --name test-topic 
----

** Create a service account and copy the service account ID and secret. You can use the service account to connect and authenticate your {product-connectors} instances with your Kafka instance.
+
[source,subs="+quotes"]
----
$ rhoas service-account create --file-format json --short-description="test-service-account" 
----

** For your Kafka instance, set the *Consume from a topic* and *Produce to a topic* permissions for the service account to `ls \*`. The `is \*` settings enable {product-connectors} instances that are configured with the service account credentials to produce and consume messages in any topic in the Kafka instance.
+
[source,subs="+quotes"]
----
$ rhoas kafka acl grant-access --producer --consumer --service-account <service-acct-id> --topic all --group all 
----

[id="proc-create-connector-namespace_{context}"]
== Creating a namespace to host your {product-connectors} instances
[role="_abstract"]

A Connectors namespace hosts your Connectors instances. 

The namespace that you use depends on your OpenShift Dedicated environment.

* If you're using a trial cluster in your own OpenShift Dedicated environment:: The namespace is created when you add the {product-connectors} service to your trial cluster, as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding and removing the Red Hat OpenShift Connectors add-on on your OpenShift Dedicated trial cluster^]. Your OSD trial cluster namespace is active for 60 days.

* *If you're using the OpenShift Connectors evaluation site*, you must create an evaluation namespace before you can create Connectors instances. An evaluation namespace is active for 48 hours.

.Prerequisites

* You're logged in to `rhoas`.
+
[source]
----
$ rhoas login
----

.Procedure

. If you're using a trial cluster in your own OSD environment, skip to Step 2.
+
If you're using the OpenShift Connectors evaluation site, create an evaluation namespace.
+
[source,subs="+quotes"]
----
$ rhoas connector namespace create --name "eval-namespace"
----

. Verify that your namespace is listed.
+
[source,subs="+quotes"]
----
$ rhoas connector namespace list
----

[id="proc-building-connector-configuration-cli_{context}"]
== Building connector configuration files

[role="_abstract"]
Before you can create a Connectors instance, you must build a configuration file that is based on a supported connector type that is listed in the {product-connectors} catalog. 

For this example, you want to create two types of connectors: a data generator (a source connector) and an HTTP sink connector.

You must build a configuration file for each connector type that you want to create. When you build a configuration file, the default file name is `connector.json`. Optionally, you can specify a different configuration file name. 

.Prerequisites

* Your current local directory is the place where you want to save your Connectors configuration files. For example, if you want to save your configuration files in a directory named `my-connectors`, ensure that the `my-connectors` directory is your current directory.
+
[source]
----
$ cd my-connectors
----

* You're logged in to `rhoas`.

* For the sink connector example, open the free https://webhook.site[Webhook.site^] page in a browser window. The Webhook.site page provides a unique URL that you can use for the example HTTP data sink.

.Procedure

. Decide which type of connector you want to create.

.. View a list of the supported connector types that are available in the Connectors catalog. The default number of connector types listed is set to 10. To see all connectors types, specify a limit value of 100.
+
[source,subs="+quotes"]
----
$ rhoas connector type list --limit=100
----
// .. Filter the list to show only sink connectors:
// +
// [source,subs="+quotes"]
// ----
// rhoas connector type list --limit=70 --search=%sink% 
// ----
// 
// .. Filter the list to show only source connectors:
// +
// [source,subs="+quotes"]
// ----
// rhoas connector type list --limit=70 --search=%source%
// ----

.. For this example, find the data generator source connector by specifying "Generator" in the `search` flag.
+
[source,subs="+quotes"]
----
$ rhoas connector type list --search=%Generator%
----
+
The result is as follows:
+
[source,subs="+quotes"]
----
{
  "name": "Data Generator source",
  "id": "data_generator_0.1",
  "description": "A data generator (for development and testing purposes)."
}
----

.. For this example, find the HTTP sink connector by specifying "HTTP" in the `search` flag.
+
[source,subs="+quotes"]
----
rhoas connector type list --search=%HTTP%
----
+
The first result is the HTTP sink.
+
[source,subs="+quotes"]
----
{
  "name": "HTTP sink",
  "id": "http_sink_0.1",
  "description": "Send data to an HTTP endpoint."
}
----

. Build a configuration file for the `data_generator_0.1` connector type. Specify `test-generator` as the Connector instance name and `test-generator.json` as the configuration file name.
+
[source,subs="+quotes"]
----
$ rhoas connector build --name=test-generator --type=data_generator_0.1 --output-file=test-generator.json
----
+
*Note:* By default, the configuration file is in JSON format. Optionally, you can specify YAML format by adding `-o yaml` to the `connector build` command.
+
You're prompted to enter details based on the data generator connector type.

.. For *Format*, press *ENTER* to accept the default (`application/octet-stream`).

.. For *Error handling method*, select `stop`. The Connector instance stops running if it encounters an error.

.. For *Topic Names*, type `test-topic`.

.. For *Content Type*, accept the default.

.. For *Message*, type `Hello World!`.

.. For *Period*, accept the default (`1000`).

. Build a configuration file for the `http_sink_0.1` connector type and specify `test-http` as the configuration file name:
+
[source,subs="+quotes"]
----
$ rhoas connector build --name=test-http --type=http_sink_0.1 --output-file=test-http.json
----
+
You're prompted to enter details based on the HTTP sink connector type.

.. For *Format*, press *ENTER* to accept the default (`application/octet-stream`).

.. For *Error handling method*, select `stop`. 

.. For *Method*, accept the default (`POST`).

.. For *URL*, paste your unique URL that you copied from the https://webhook.site[Webhook.site^] page. 

.. For *Topic Names*, type `test-topic`.

. Verify that the configuration files were built.
+
[source]
----
$ ls
----
+
The result shows the `test-generator.json` and `test-http.json` files.

. Optionally, you can edit a configuration file in an editor of your choice.
+
*Note:* To prevent saving sensitive data to disk, the values for the service account and the namespace are not included in the configuration file. You're prompted to specify those values when you create a {product-connectors} instance.

[id="proc-create-connector-instances_{context}"]
== Creating Connectors instances
[role="_abstract"]

After you build a configuration file based on a connector type, you can use the configuration file to create a Connectors instance.

For this example, you create two Connectors instances: a data generator source Connectors instance and an HTTP sink connectors instance.

.Prerequisites

* You have built a Connectors configuration files based on each type of connector that you want to create and the configuration files are saved in your current directory.
* You have a Connectors namespace.
* You have an {product-long-kafka} instance running and have a topic called `test-topic`.
* You have a service account created that has read and write access to the Kafka topic, and you know the credentials (ID and secret).

.Procedure

. Create a source Connectors instance by specifying the source connector's configuration file. For example, the data generator configuration file is `test-generator.json`.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=test-generator.json 
----
+
You're prompted to provide details for the Connectors instance.

.. For *Set the Connectors namespace*, select your namespace from the list. For example, select `eval-namespace`.

.. For *Service Account Client ID*, type or paste your ID.

.. For *Service Account Client Secret*, type or paste your secret.
+ 
A message states "Successfully created the Connectors instance".

.. Wait until the status of the Connectors instance is *Ready*. 
+
To check the status:
+
[source,subs="+quotes"]
----
$ rhoas connector list
----

.. Verify that the your source Connectors instance is producing messages.

. Create a sink Connectors instance by specifying the sink connector's configuration file. For example, the HTTP sink configuration file is `test-http.json`.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=test-http.json 
----
+
You're prompted to provide details for the Connectors instance.

.. For *Set the Connectors namespace*, select your namespace from the list. For example, select `eval-namespace`.

.. For *Service Account Client ID*, type or paste your ID.

.. For *Service Account Client Secret*, type or paste your secret.
+
A message states "Successfully created the Connectors instance".

.. Wait until the status of the Connectors instance is *Ready*. 
+
To check the status:
+
[source,subs="+quotes"]
----
$ rhoas connector list
----

. Verify that the your sink Connectors instance is receiving messages by viewing your link:https://webhook.site[Webhook.site^] page in a web browser.


[role="_additional-resources"]
.Additional resources
* To access the `rhoas connector` help page, type `rhoas connector -h`
{base-url-cli}{command-ref-url-cli}[_CLI command reference (rhoas)_^]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]