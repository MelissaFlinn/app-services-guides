////
START GENERATED ATTRIBUTES
WARNING: This content is generated by running npm --prefix .build run generate:attributes
////

//All OpenShift Application Services
:org-name: Application Services
:product-long-rhoas: OpenShift Application Services
:community:
:imagesdir: ./images
:property-file-name: app-services.properties
:samples-git-repo: https://github.com/redhat-developer/app-services-guides
:base-url: https://github.com/redhat-developer/app-services-guides/tree/main/docs/
:sso-token-url: https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token
:cloud-console-url: https://console.redhat.com/
:service-accounts-url: https://console.redhat.com/application-services/service-accounts

//OpenShift Application Services CLI
:base-url-cli: https://github.com/redhat-developer/app-services-cli/tree/main/docs/
:command-ref-url-cli: commands
:installation-guide-url-cli: rhoas/rhoas-cli-installation/README.adoc
:service-contexts-url-cli: rhoas/rhoas-service-contexts/README.adoc

//OpenShift Streams for Apache Kafka
:product-long-kafka: OpenShift Streams for Apache Kafka
:product-kafka: Streams for Apache Kafka
:product-version-kafka: 1
:service-url-kafka: https://console.redhat.com/application-services/streams/
:getting-started-url-kafka: kafka/getting-started-kafka/README.adoc
:kafka-bin-scripts-url-kafka: kafka/kafka-bin-scripts-kafka/README.adoc
:kafkacat-url-kafka: kafka/kcat-kafka/README.adoc
:quarkus-url-kafka: kafka/quarkus-kafka/README.adoc
:nodejs-url-kafka: kafka/nodejs-kafka/README.adoc
:getting-started-rhoas-cli-url-kafka: kafka/rhoas-cli-getting-started-kafka/README.adoc
:topic-config-url-kafka: kafka/topic-configuration-kafka/README.adoc
:consumer-config-url-kafka: kafka/consumer-configuration-kafka/README.adoc
:access-mgmt-url-kafka: kafka/access-mgmt-kafka/README.adoc
:metrics-monitoring-url-kafka: kafka/metrics-monitoring-kafka/README.adoc
:service-binding-url-kafka: kafka/service-binding-kafka/README.adoc
:message-browsing-url-kafka: kafka/message-browsing-kafka/README.adoc

//OpenShift Service Registry
:product-long-registry: OpenShift Service Registry
:product-registry: Service Registry
:registry: Service Registry
:product-version-registry: 1
:service-url-registry: https://console.redhat.com/application-services/service-registry/
:getting-started-url-registry: registry/getting-started-registry/README.adoc
:quarkus-url-registry: registry/quarkus-registry/README.adoc
:getting-started-rhoas-cli-url-registry: registry/rhoas-cli-getting-started-registry/README.adoc
:access-mgmt-url-registry: registry/access-mgmt-registry/README.adoc
:content-rules-registry: https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817[Supported Service Registry content and rules]
:service-binding-url-registry: registry/service-binding-registry/README.adoc

//OpenShift Connectors
:product-long-connectors: OpenShift Connectors
:product-connectors: Connectors
:product-version-connectors: 1
:service-url-connectors: https://console.redhat.com/application-services/connectors
:getting-started-url-connectors: connectors/getting-started-connectors/README.adoc
<<<<<<< HEAD
=======
:getting-started-rhoas-cli-url-connectors: connectors/rhoas-cli-getting-started-connectors/README.adoc
>>>>>>> b8d6eef (jc-1084 edit connectors CLI get start)

//OpenShift API Designer
:product-long-api-designer: OpenShift API Designer
:product-api-designer: API Designer
:product-version-api-designer: 1
:service-url-api-designer: https://console.redhat.com/application-services/api-designer/
:getting-started-url-api-designer: api-designer/getting-started-api-designer/README.adoc

//OpenShift API Management
:product-long-api-management: OpenShift API Management
:product-api-management: API Management
:product-version-api-management: 1
:service-url-api-management: https://console.redhat.com/application-services/api-management/

////
END GENERATED ATTRIBUTES
////

[id="chap-connectors-rhoas-cli"]
<<<<<<< HEAD
= Interacting with {product-long-connectors} the rhoas CLI
=======
= Getting started with the rhoas CLI for {product-long-connectors}
>>>>>>> b8d6eef (jc-1084 edit connectors CLI get start)
ifdef::context[:parent-context: {context}]
:context: connectors-rhoas-cli

// Purpose statement for the assembly
[role="_abstract"]
<<<<<<< HEAD
As a developer of {product-connectors}, you can use the `rhoas` command-line interface (CLI) to control and manage your Connectors instances and namespaces.

.Prerequisites
ifndef::community[]
* You have a Red Hat account.
endif::[]
* You have a running Kafka instance in {product-kafka} with a topic called `my-topic`.
* You've installed the latest version of the `rhoas` CLI. See {base-url}{installation-guide-url-cli}[Installing and configuring the rhoas CLI^].

// Condition out QS-only content so that it doesn't appear in docs.
// All QS anchor IDs must be in this alternate anchor ID format `[#anchor-id]` because the ascii splitter relies on the other format `[id="anchor-id"]` to generate module files.
ifdef::qs[]
[#description]
====
Learn how to use the `rhoas` command-line interface (CLI) to produce and consume messages for a Kafka instance.
====

[#introduction]
====
Welcome to the quick start for producing and consuming Kafka messages using the `rhoas` command-line interface (CLI).

In this quick start, you'll use a CLI command to produce messages to different topic partitions in a Kafka instance. You'll then use the {product-long-kafka} web console to inspect the messages. When you're ready, you'll use another CLI command to consume the messages.
====
endif::[]

[id="proc-building-connector-configuration{context}"]
== Building a connectors configuration 

[role="_abstract"]
In this task you will create a configuration file that can be used to create a {product-long-connectors} instance.

.Procedure
. Log in to the `rhoas` CLI.
=======
As a developer of {product-connectors}, you can use the `rhoas` command-line interface (CLI) to create and configure connections between {product-long-kafka} and third-party systems.

A *source* connector allows you to send data from an external system to {product-kafka}. A *sink* connector allows you to send data from {product-kafka} to an external system.

For the example in this guide, you create a source connector that sends data from a simple data generator to a Kafka topic. You also create a sink connector that sends data from the Kafka topic to an HTTP site.

This guide describes how to:

* {base-url}{getting-started-rhoas-cli-url-connectors}#proc-create-connector-namespace_connectors-rhoas-cli[Create a namespace to host your {product-connectors} instances.]
* {base-url}{getting-started-rhoas-cli-url-connectors}#proc-building-connector-configuration-cli_connectors-rhoas-cli[Build a configuration file for each type of connector that you want to create]
* Create a Connectors instance by specifying a configuration file.
* Manage your Connectors instances (start, stop, update, and delete).

.Prerequisites

* You have a Red Hat account.
* You've installed the latest version of the `rhoas` CLI. See link:{base-url}{installation-guide-url-cli}[Installing and configuring the rhoas CLI^].
* You have completed the following tasks:
+
*Note:* You can find detailed instructions for these tasks in link:{base-url}{getting-started-rhoas-cli-url-kafka}[Getting started with the rhoas CLI for {product-long-kafka}^].
+
** Create a Kafka instance
[source,subs="+quotes"]
----
$ rhoas kafka create --name my-kafka-instance 
----
+
And the Kafka instance is in the *Ready* state.
+
[source,subs="+quotes"]
----
$ rhoas context status kafka 
----

** Create a Kafka topic named `test-topic`. The Kafka topic stores messages sent by producers (data sources) and makes them available to consumers (data sinks).
+
[source,subs="+quotes"]
----
$ rhoas kafka topic create --name test-topic 
----

** Create a service account and copy the service account ID and secret. The service account allows you to connect and authenticate your Connectors instances with your Kafka instance.
+
[source,subs="+quotes"]
----
$ rhoas service-account create --file-format json --short-description="test-service-account" 
----
+
[source,subs="+quotes"]
----
Service account created successfully with ID "e1310a8f-8267-4a52-9ce0-bdf326b76875"
✔️  Credentials saved to /Users/mflinn/mylocalGit/app-services-cli/credentials.json
----

** In your Kafka instance, set the *Consume from a topic* and *Produce to a topic* permissions for the service account to `ls *`. (The `is "*"` settings enable Connectors instances that are configured with the service account credentials to produce and consume messages in any topic in the Kafka instance.)
+
[source,subs="+quotes"]
----
$ rhoas kafka acl grant-access --producer --consumer --service-account <service-acct-id> --topic all --group all 
----

[id="proc-create-connector-namespace_{context}"]
== Creating a namespace to host your {product-connectors} instances
[role="_abstract"]

A Connectors namespace hosts your Connectors instances. 

The namespace that you use depends on your OpenShift Dedicated environment.

* *If you're using a trial cluster in your own OpenShift Dedicated environment*, the namespace is created when you add the Connectors service to your trial cluster, as described in https://access.redhat.com/documentation/en-us/openshift_connectors/1/guide/15a79de0-8827-4bf1-b445-8e3b3eef7b01[Adding and removing the Red Hat OpenShift Connectors add-on on your OpenShift Dedicated trial cluster^]. Your OSD trial cluster namespace is active for 60 days.

* *If you're using the OpenShift Connectors evaluation site*, you must create an evaluation namespace before you can create Connectors instances. An evaluation namespace is active for 48 hours.

.Prerequisites

* You are logged in to `rhoas`.
>>>>>>> b8d6eef (jc-1084 edit connectors CLI get start)
+
[source]
----
$ rhoas login
----

<<<<<<< HEAD
. Start building a configuration for a {{product-long-connectors} instance.
+
[source,subs="+quotes"]
----
$ rhoas connector build --type=data_generator_0.1
----

You're prompted to enter details based on the Connectors instance type provided.

. Enter `my-topic` as the topic names value.

. Accept the default Content Type by pressing enter when prompted.

. Enter `Hello World!` as the message value.

. Accept the default Period.

.Verification
ifdef::qs[]
* Is there a file called `connector.json` in the current working directory?
endif::[]
ifndef::qs[]
* Verify that the connector specific configuration is set in the `connector.json` file.
endif::[]

[id="proc-create-connector{context}"]
== Creating a {product-long-connectors} from Rhoas CLI
[role="_abstract"]
Once you have build a configuration you can create the Connectors instance from the Rhoas CLI.

.Prerequisites
* You have buillt a Connectors configuration and it is saved locally as `connectors.json`.
* You have a {product-long-kafka} instance running and have a topic called `my-topic`.
* You have a service account created that has read and write access to the kafka topic and you have the credentials saved for use.

.Procedure
. Create a evaluation namespace that the connectors instance will use.
+
[source,subs="+quotes"]
----
$ rhoas namespace create
----

. Run the create command and pass in the current configuration.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=connector.json 
----

. You're prompted to enter details to create the {product-connectors} instance.

+
Enter `my-connector` as the connector's name.
+
Select the namespace you created when prompted.
+
If prompted select the kafka instance you created with the topic you want to send messages too.
+
Enter the Service Account Client Id when prompted.
+
Enter the Service Account Client Secret when prompted.

. Once the {product-long-connectors} is running run the following command to see if the connector is producing messages as configured.

[source,subs="+quotes"]
----
$ rhoas kafka topic consume --name=my-topic --partition=0 --wait
----

.Verification
* Is there a connectors instance running called `my-connector`.
* Is there messages being received as expected? 

ifdef::qs[]
[#conclusion]
====
Congratulations! You successfully completed the quick start for creating a connectors instance with the `rhoas` CLI.
====
endif::[]
=======
.Procedure

. If you're using a trial cluster in your own OSD environment, skip to the next step.
+
If you're using the OpenShift Connectors evaluation site, create an evaluation namespace.
+
[source,subs="+quotes"]
----
$ rhoas connector namespace create --name "eval-namespace"
----

. Verify that your namespace is listed.
+
[source,subs="+quotes"]
----
$ rhoas connector namespace list
----

[id="proc-building-connector-configuration-cli_{context}"]
== Building connector configuration files

[role="_abstract"]
Before you can create a Connectors instance, you need to build a configuration file that is based on a supported connector type that is listed in the {product-connectors} catalog. 

For this example, you want to create two types of connectors: a data generator (a source connector) and an HTTP sink connector.

You must build a configuration file for each connector type that you want to create. When you build a configuration file, the default filename is `connector.json`. Optionally, you can specify a different configuration filename. 

.Prerequisites

* For the sink connector example, open the free link:https://webhook.site[Webhook.site^] in a browser window. The Webhook site page provides a unique URL that you copy. You use this URL as an HTTP data sink.
* Your current local directory is the place where you want to save your Connectors configuration files. For example. if you want to save your configuration files in a directory named `my-connectors`, make it the current directory.
+
[source]
----
$ cd my-connectors
----

* You are logged in to `rhoas`.
* You have a Connectors namespace.
* You have a {product-long-kafka} instance running and have a topic called `test-topic`.
* You have a service account created that has read and write access to the Kafka topic and you have the credentials (ID and secret) saved for use.

.Procedure

. Decide which type of connector you want to create.

.. View a list of the supported connector types that are available in the Connectors catalog. The default number of connector types is set to 10. To see all connectors types, specify as limit value of 70.
+
[source,subs="+quotes"]
----
rhoas connector type list --limit=70
----

// .. Filter the list to show only sink connectors:
// +
// [source,subs="+quotes"]
// ----
// rhoas connector type list --limit=70 --search=%sink% 
// ----
// 
// .. Filter the list to show only source connectors:
// +
// [source,subs="+quotes"]
// ----
// rhoas connector type list --limit=70 --search=%source%
// ----


. For this example, find the data generator source connector by specifying "Data" in the `search` flag.
+
[source,subs="+quotes"]
----
$ rhoas connector type list --search=%Generator%
----
+
The result is:
+
[source,subs="+quotes"]
----
{
  "name": "Data Generator source",
  "id": "data_generator_0.1",
  "description": "A data generator (for development and testing purposes)."
}
----

. For this example, find the HTTP sink connector by specifying "HTTP" in the `search` flag.
+
[source,subs="+quotes"]
----
rhoas connector type list --search=%HTTP%
----
+
The first result is
+
[source,subs="+quotes"]
----
{
  "name": "HTTP sink",
  "id": "http_sink_0.1",
  "description": "Send data to a HTTP endpoint."
}
----

. Build a configuration file for the `data_generator_0.1` connector type and specify `test-generator` as the Connector instance name and `test-generator.json` as the configuration file name:
+
[source,subs="+quotes"]
----
$ rhoas connector build --name=test-generator --type=data_generator_0.1 --output-file=test-generator.json
----
+
*Note:* By default, the configuration file is in JSON format. Optionally, you can specify YAML format by adding `-o yaml` to the `connector build` command.
+
You're prompted to enter details based on the data generator connector type.

.. For *Enter a value for format*, press *ENTER* to accept the default (`application/octet-stream`).

.. For *error_handler*, accept the default (`log`).

.. For *Topic Names*, type `test-topic`.

.. For *Content Type*, accept the default.

.. For *Message*, type `Hello World!`.

.. For *Period*, accept the default (`1000`).

. Build a configuration file for the `http_sink_0.1` connector type and specify `test-http` as the configuration file name:
+
[source,subs="+quotes"]
----
$ rhoas connector build --name=test-http --type=http_sink_0.1 --output-file=test-http.json
----
+
You're prompted to enter details based on the HTTP sink connector type.

.. For *topic names*, type `test-topic`.

.. For *Content Type*, accept the default by pressing *ENTER*.

.. For the *URL*, paste your unique URL that you copied from the link:https://webhook.site[Webhook.site^]. 

.. For the *Period*, accept the default.

. Verify that the configuration files were built
+
[source]
----
$ ls
----
+
The result shows the `test-generator.json` and `test-http.json` files.

. Optionally, you can edit a configuration file in an editor of your choice.
+
*Note:* You do not need to specify values for the service account or the namespace in the configuration file. You can specify those values when you create a Connectors instance.

[id="proc-create-connector-instances_{context}"]
== Creating Connectors instances
[role="_abstract"]

After you build a configuration file based on a connector type, you can use the configuration file to create a Connectors instance.

For this example, you create two Connectors instances: a data generator source Connectors instance and an HTTP sink connectors instance.

.Prerequisites

* You have built a Connectors configuration files based on each type of connector that you want to create and the configuration files are saved in your current directory.
* You have a Connectors namespace.
* You have a {product-long-kafka} instance running and have a topic called `test-topic`.
* You have a service account created that has read and write access to the Kafka topic and you know the credentials (ID and secret).

.Procedure

. Create a source Connectors instance by specify the source connector's configuration file. For example, the data generator configuration file is `test-generator.json`.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=test-generator.json 
----
+
You're prompted to provide details for the Connectors instance.

.. For *Set the Connectors namespace*, select your namespace from the list. For example, select `eval-namespace`.

.. For *Service Account Client ID*, type or paste your ID.

.. For *Service Account Client Secret*, type or paste your secret.
+ 
A message states "Successfully created the Connectors instance".

.. Wait until the status of the Connectors instance is *Ready*. 
+
To check the status:
+
[source,subs="+quotes"]
----
$ rhoas connector list
----

.. Verify that the your source Connectors instance is producing messages.

. Create a sink Connectors instance by specify the sink connector's configuration file. For example, the HTTP sink configuration file is `test-http.json`.
+
[source,subs="+quotes"]
----
$ rhoas connector create --file=test-http.json 
----
+
You're prompted to provide details for the Connectors instance.

.. For *Set the Connectors namespace*, select your namespace from the list. For example, select `eval-namespace`.

.. For *Service Account Client ID*, type or paste your ID.

.. For *Service Account Client Secret*, type or paste your secret.

A message states "Successfully created the Connectors instance".

. Wait until the status of the Connectors instance is *Ready*. 
+
To check the status:
+
[source,subs="+quotes"]
----
$ rhoas connector list
----

. Verify that the your sink Connectors instance is receiving messages by viewing your webhook site page in a your web browser.



//== Commands for managing Connectors
//copied from Kakfa - need to rewrite

//[role="_abstract"]
//For more information about the `rhoas` commands you can use to manage your Kafka instance,
//use the following command help:

//* `rhoas kafka -h` for Kafka instances
//* `rhoas service-account -h` for service accounts
//* `rhoas kafka acl -h` for access management
//* `rhoas kafka topic -h` for Kafka topics

[role="_additional-resources"]
.Additional resources
* {base-url-cli}{command-ref-url-cli}[_CLI command reference (rhoas)_^]
>>>>>>> b8d6eef (jc-1084 edit connectors CLI get start)

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]